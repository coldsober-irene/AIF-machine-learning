{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/AIF-machine-learning/blob/main/HAR_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFGs2OuxCbtk"
      },
      "source": [
        "##metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ScftYlmzCxnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7976eb92-d86f-4a28-c5f9-1ac0e26a8f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6xOtQ63pph2"
      },
      "source": [
        "##packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kuVnAbZuprff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O2H0bPEilw9"
      },
      "source": [
        "# 1. **Frames sampling**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 20\n",
        "rate = 7"
      ],
      "metadata": {
        "id": "HLpKYITKsFuf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "YtnZh48mby_P"
      },
      "outputs": [],
      "source": [
        "class Sampling:\n",
        "  count = 0\n",
        "  def __init__(self, base_dir, sampling_type = 'uniform', ref_mean=[0.07, 0.07, 0.07], ref_std=[0.1, 0.09, 0.08]):\n",
        "    self.data_path = base_dir\n",
        "    self.sampling_type = sampling_type\n",
        "    self.mean = ref_mean\n",
        "    self.std = ref_std\n",
        "\n",
        "    # READ MAPPING FILE TO KNOW THE LABEL FOR EACH CLASS\n",
        "    map_file = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/mapping_table_23.txt'\n",
        "    self.maps = {}\n",
        "    with open(map_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.split()\n",
        "            self.maps[parts[1]] = int(parts[0])\n",
        "\n",
        "    # CREATE EXTRACTOR OBJECT\n",
        "    self.Extractor = self.Feature_extract(sampled_type = self.sampling_type)\n",
        "\n",
        "    # subfolders\n",
        "    self.activities = os.listdir(self.data_path)\n",
        "\n",
        "    # EXTRACTED FEATURES FROM ALL THE VIDEOS\n",
        "    self.obtained_features = []\n",
        "    # LABELS OF THE EXTRACTED FEATURES\n",
        "    self.labels = []\n",
        "    Sampling.count += 1\n",
        "\n",
        "  def saveFeatures(self):\n",
        "    # CREATE VSTACK ARRAY OF ALL FEATURES EXTRACTED\n",
        "    all_features = np.vstack(self.obtained_features)\n",
        "    labels = np.array(self.labels)\n",
        "\n",
        "    # SAVED THE EXTRACTED FEATURES and their corresponding labels FOR FUTURE USE\n",
        "    features_dir = os.path.join(self.data_path, 'features')\n",
        "    os.makedirs(features_dir, exist_ok = True)\n",
        "\n",
        "    np.save(os.path.join(features_dir,f'features{Sampling.count}.npy'), all_features)\n",
        "    np.save(os.path.join(features_dir,f'labels{Sampling.count}.npy'), labels)\n",
        "\n",
        "    print(\"FEATURE EXTRACTION AND SAVING IS DONE!!!\")\n",
        "\n",
        "\n",
        "  def Sampler(self, sample_rate = 5, num_samples = 10):\n",
        "    # Loop through each activity\n",
        "    for activity in self.activities:\n",
        "        activity_folder = os.path.join(self.data_path, activity)\n",
        "\n",
        "        # Loop through video files in the activity folder\n",
        "        for video_file in os.listdir(activity_folder):\n",
        "            if '.mp4' in video_file:\n",
        "              frames_sampled = []\n",
        "              video_path = os.path.join(activity_folder, video_file)\n",
        "              cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "              frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "              if self.sampling_type == 'uniform':\n",
        "                sample = self.UniformSampling(cap = cap, frameCount=frame_count, sample_rate = sample_rate)\n",
        "                frames_sampled.append(sample)\n",
        "\n",
        "              elif self.sampling_type == 'random':\n",
        "                sample = self.RandomSampling(cap = cap, frameCount=frame_count, num_samples=num_samples)\n",
        "                frames_sampled.append(sample)\n",
        "\n",
        "              # EXTRACT FEATURE FROM THE FRAMES OF EACH VIDEO\n",
        "              features_obtained = self.Extractor.features(frames = frames_sampled, ref_mean = self.mean, ref_std = self.std)\n",
        "              self.obtained_features.append(features_obtained)\n",
        "\n",
        "              # POPULATE THE LABEL CORRESPONDING TO THE CURRENT VIDEO\n",
        "              self.labels.append(self.maps[activity])\n",
        "\n",
        "  def UniformSampling(self, cap, sample_rate, frameCount):\n",
        "    for i in range(0, frameCount, sample_rate):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "          return frame\n",
        "\n",
        "  def RandomSampling(self, cap,num_samples, frameCount):\n",
        "    sampled_indices = random.sample(range(frameCount), num_samples)\n",
        "\n",
        "    for i in sampled_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            return frame\n",
        "\n",
        "  class Feature_extract:\n",
        "    def __init__(self, sampled_type = 'uniform'):\n",
        "      # Load pre-trained ResNet50\n",
        "      self.model = ResNet50(weights='imagenet', include_top=False)\n",
        "      self.sampled_type = sampled_type\n",
        "\n",
        "    # Function to normalize a frame\n",
        "    def normalize_frame(self, frame, ref_mean, ref_std):\n",
        "        actual_mean = np.mean(frame, axis=(0, 1), keepdims=True)\n",
        "        actual_std = np.std(frame, axis=(0, 1))\n",
        "        normalized_frame = (frame - actual_mean) / actual_std * ref_std + ref_mean\n",
        "        return normalized_frame\n",
        "\n",
        "    # Function to preprocess frames and extract features using ResNet\n",
        "    def features(self,frames, ref_mean, ref_std):\n",
        "        processed_frames = [self.normalize_frame(frame, ref_mean, ref_std) for frame in frames]\n",
        "        processed_frames = [preprocess_input(frame) for frame in processed_frames]\n",
        "        features = self.model.predict(np.array(processed_frames))\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN EXTRACTION OF THE TRAIN DATASETS"
      ],
      "metadata": {
        "id": "oradMywtTP-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "uniform sampling"
      ],
      "metadata": {
        "id": "ZGUp-vXDneT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lA9YGGIoifBY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "data_root = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train'\n",
        "sample = Sampling(base_dir = data_root)\n",
        "sample.Sampler(sample_rate = rate)\n",
        "sample.saveFeatures()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "random sampling"
      ],
      "metadata": {
        "id": "U8_SZNAeniHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "sample2 = Sampling(base_dir = data_root, sampling_type = 'random')\n",
        "sample2.Sampler(num_samples=samples)\n",
        "sample2.saveFeatures()"
      ],
      "metadata": {
        "id": "4aEH6mIpnkQD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae0E0pcRlKK9"
      },
      "source": [
        "#2. **validation data feature extraction**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling:\n",
        "  count = 0\n",
        "  def __init__(self, base_dir, sampling_type = 'uniform', ref_mean=[0.07, 0.07, 0.07], ref_std=[0.1, 0.09, 0.08]):\n",
        "    self.data_path = base_dir\n",
        "    self.sampling_type = sampling_type\n",
        "    self.mean = ref_mean\n",
        "    self.std = ref_std\n",
        "\n",
        "    # READ MAPPING FILE TO KNOW THE LABEL FOR EACH CLASS\n",
        "    map_file = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate.txt'\n",
        "    self.maps = {}\n",
        "    with open(map_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.split()\n",
        "            self.maps[parts[-1]] = int(parts[1])\n",
        "\n",
        "    # CREATE EXTRACTOR OBJECT\n",
        "    self.Extractor = self.Feature_extract(sampled_type = self.sampling_type)\n",
        "\n",
        "\n",
        "    # EXTRACTED FEATURES FROM ALL THE VIDEOS\n",
        "    self.obtained_features = []\n",
        "    # LABELS OF THE EXTRACTED FEATURES\n",
        "    self.labels = []\n",
        "    Sampling.count += 1\n",
        "\n",
        "  def saveFeatures(self):\n",
        "    # CREATE VSTACK ARRAY OF ALL FEATURES EXTRACTED\n",
        "    all_features = np.vstack(self.obtained_features)\n",
        "    labels = np.array(self.labels)\n",
        "\n",
        "    # SAVED THE EXTRACTED FEATURES and their corresponding labels FOR FUTURE USE\n",
        "    features_dir = os.path.join(self.data_path, 'features')\n",
        "    os.makedirs(features_dir, exist_ok = True)\n",
        "\n",
        "    np.save(os.path.join(features_dir,f'features{Sampling.count}.npy'), all_features)\n",
        "    np.save(os.path.join(features_dir,f'labels{Sampling.count}.npy'), labels)\n",
        "\n",
        "    print(\"VALIDATION FEATURE EXTRACTION AND SAVING IS DONE!!!\")\n",
        "\n",
        "\n",
        "  def Sampler(self, sample_rate = 5, num_samples = 10):\n",
        "    # Loop through video files in the activity folder\n",
        "    for video_file in os.listdir(self.data_path):\n",
        "        if '.mp4' in video_file:\n",
        "          frames_sampled = []\n",
        "          video_path = os.path.join(self.data_path, video_file)\n",
        "          cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "          frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "          if self.sampling_type == 'uniform':\n",
        "            sample = self.UniformSampling(cap = cap, frameCount=frame_count, sample_rate = sample_rate)\n",
        "            frames_sampled.append(sample)\n",
        "\n",
        "          elif self.sampling_type == 'random':\n",
        "            sample = self.RandomSampling(cap = cap, frameCount=frame_count, num_samples=num_samples)\n",
        "            frames_sampled.append(sample)\n",
        "\n",
        "          # EXTRACT FEATURE FROM THE FRAMES OF EACH VIDEO\n",
        "          features_obtained = self.Extractor.features(frames = frames_sampled, ref_mean = self.mean, ref_std = self.std)\n",
        "          self.obtained_features.append(features_obtained)\n",
        "\n",
        "          # POPULATE THE LABEL CORRESPONDING TO THE CURRENT VIDEO\n",
        "          self.labels.append(self.maps[video_file])\n",
        "\n",
        "  def UniformSampling(self, cap, sample_rate, frameCount):\n",
        "    for i in range(0, frameCount, sample_rate):\n",
        "      cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "      ret, frame = cap.read()\n",
        "      if ret:\n",
        "          return frame\n",
        "\n",
        "  def RandomSampling(self, cap,num_samples, frameCount):\n",
        "    sampled_indices = random.sample(range(frameCount), num_samples)\n",
        "\n",
        "    for i in sampled_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            return frame\n",
        "\n",
        "  class Feature_extract:\n",
        "    def __init__(self, sampled_type = 'uniform'):\n",
        "      # Load pre-trained ResNet50\n",
        "      self.model = ResNet50(weights='imagenet', include_top=False)\n",
        "      self.sampled_type = sampled_type\n",
        "\n",
        "    # Function to normalize a frame\n",
        "    def normalize_frame(self, frame, ref_mean, ref_std):\n",
        "        actual_mean = np.mean(frame, axis=(0, 1), keepdims=True)\n",
        "        actual_std = np.std(frame, axis=(0, 1))\n",
        "        normalized_frame = (frame - actual_mean) / actual_std * ref_std + ref_mean\n",
        "        return normalized_frame\n",
        "\n",
        "    # Function to preprocess frames and extract features using ResNet\n",
        "    def features(self,frames, ref_mean, ref_std):\n",
        "        processed_frames = [self.normalize_frame(frame, ref_mean, ref_std) for frame in frames]\n",
        "        processed_frames = [preprocess_input(frame) for frame in processed_frames]\n",
        "        features = self.model.predict(np.array(processed_frames))\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "WQXBHWMVPM1o"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN EXTRACTION OF THE VALIDATION DATASETS"
      ],
      "metadata": {
        "id": "rKm-nK2YTbEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "uniform sampling"
      ],
      "metadata": {
        "id": "siTOtg_AnGxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "val_root = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate'\n",
        "sample_val = Sampling(base_dir = val_root)\n",
        "sample_val.Sampler(sample_rate = rate)\n",
        "sample_val.saveFeatures()"
      ],
      "metadata": {
        "id": "ExyLbFRVD_-K"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random sampling"
      ],
      "metadata": {
        "id": "7ebv2fDhnJmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "sample_val2 = Sampling(base_dir = val_root, sampling_type = 'random')\n",
        "sample_val2.Sampler(num_samples=samples)\n",
        "sample_val2.saveFeatures()"
      ],
      "metadata": {
        "id": "mhMRmAtCnL4i"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_6i4iD8Jo4"
      },
      "source": [
        "### obtain the features of each frame using a pre-trained model and create feature vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsrzZaJ4WrKZ"
      },
      "source": [
        "## fuse features extracted using average pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxDZcyULVVun"
      },
      "source": [
        "## Question: ***Describe in brief the pre-trained model leveraged and why the pre-trained model is selected. What is the dimension of the feature obtained. Remember to save the video features in order for subsequent training. (3 points)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgDGUCt2XiXD"
      },
      "source": [
        "# 3. **Classifier Training and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clRJUkrjdEKO"
      },
      "source": [
        "**possible classifier I can choose from since my datasets are small (just 25 videos)**\n",
        "\n",
        "1.   Support Vector Machines (SVM)\n",
        "2.   Naive Bayes\n",
        "3. Random Forest\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ctsv1c1CJPgx"
      },
      "outputs": [],
      "source": [
        "val_base = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate'\n",
        "feature_dir = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train/features/features1.npy'\n",
        "labels_dir = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train/features/labels1.npy'\n",
        "val_features = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/features/features1.npy'\n",
        "val_labels = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/features/labels1.npy'\n",
        "X_train = np.load(feature_dir, allow_pickle = True)\n",
        "y_train = np.load(labels_dir, allow_pickle = True)\n",
        "X_val = np.load(val_features, allow_pickle = True)\n",
        "y_val = np.load(val_labels, allow_pickle = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.64\n",
        "shuffle = True\n",
        "num_classes = 6\n",
        "epoch = 30"
      ],
      "metadata": {
        "id": "lVoW4Wrmtvqb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.vstack([X_train, X_val])\n",
        "# print(y_train)\n",
        "# print(y_val)\n",
        "labels = np.hstack([y_train, y_val])\n",
        "# print(labels.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=test_size, shuffle = shuffle, random_state=42)"
      ],
      "metadata": {
        "id": "kwOksaJDkLNo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model training under **uniform** sampled data"
      ],
      "metadata": {
        "id": "y7Hy3QXVp-Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model with 3D convolutional layers\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=X_train.shape[1:]),  # Input shape matches your feature shape\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')  # Output layer with the number of classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=epoch, validation_data=(X_val, y_val))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "RTl85koSu5ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28f9c2a-f301-4446-a141-709e7228e38a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3/3 [==============================] - 2s 466ms/step - loss: 2.9101 - accuracy: 0.2386 - val_loss: 3.5871 - val_accuracy: 0.1709\n",
            "Epoch 2/30\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 3.0335 - accuracy: 0.1250 - val_loss: 2.2799 - val_accuracy: 0.1646\n",
            "Epoch 3/30\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 2.0397 - accuracy: 0.2273 - val_loss: 2.0367 - val_accuracy: 0.1709\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 1.9926 - accuracy: 0.1250 - val_loss: 1.8627 - val_accuracy: 0.1835\n",
            "Epoch 5/30\n",
            "3/3 [==============================] - 1s 554ms/step - loss: 1.8889 - accuracy: 0.1250 - val_loss: 1.8710 - val_accuracy: 0.1456\n",
            "Epoch 6/30\n",
            "3/3 [==============================] - 1s 496ms/step - loss: 1.8095 - accuracy: 0.2273 - val_loss: 1.8464 - val_accuracy: 0.1772\n",
            "Epoch 7/30\n",
            "3/3 [==============================] - 1s 500ms/step - loss: 1.8159 - accuracy: 0.2045 - val_loss: 1.8445 - val_accuracy: 0.1772\n",
            "Epoch 8/30\n",
            "3/3 [==============================] - 1s 423ms/step - loss: 1.7797 - accuracy: 0.2273 - val_loss: 1.9006 - val_accuracy: 0.1456\n",
            "Epoch 9/30\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 1.7715 - accuracy: 0.2273 - val_loss: 1.7947 - val_accuracy: 0.2848\n",
            "Epoch 10/30\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 1.7608 - accuracy: 0.2273 - val_loss: 1.7777 - val_accuracy: 0.1772\n",
            "Epoch 11/30\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 1.7159 - accuracy: 0.2386 - val_loss: 1.8076 - val_accuracy: 0.1456\n",
            "Epoch 12/30\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 1.7062 - accuracy: 0.2500 - val_loss: 1.7732 - val_accuracy: 0.2215\n",
            "Epoch 13/30\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 1.6855 - accuracy: 0.3182 - val_loss: 1.7776 - val_accuracy: 0.2722\n",
            "Epoch 14/30\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 1.6990 - accuracy: 0.2955 - val_loss: 1.7760 - val_accuracy: 0.2595\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - 1s 291ms/step - loss: 1.6932 - accuracy: 0.2727 - val_loss: 1.7811 - val_accuracy: 0.1772\n",
            "Epoch 16/30\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 1.6735 - accuracy: 0.2727 - val_loss: 1.7608 - val_accuracy: 0.2025\n",
            "Epoch 17/30\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 1.6442 - accuracy: 0.3295 - val_loss: 1.7479 - val_accuracy: 0.1899\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - 1s 296ms/step - loss: 1.6326 - accuracy: 0.3295 - val_loss: 1.7562 - val_accuracy: 0.1582\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 1.6372 - accuracy: 0.2500 - val_loss: 1.7398 - val_accuracy: 0.2215\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 1.5994 - accuracy: 0.3295 - val_loss: 1.7461 - val_accuracy: 0.2152\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - 1s 504ms/step - loss: 1.6211 - accuracy: 0.3182 - val_loss: 1.7240 - val_accuracy: 0.2089\n",
            "Epoch 22/30\n",
            "3/3 [==============================] - 1s 516ms/step - loss: 1.5922 - accuracy: 0.3750 - val_loss: 1.7453 - val_accuracy: 0.1709\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - 1s 581ms/step - loss: 1.5875 - accuracy: 0.2727 - val_loss: 1.7231 - val_accuracy: 0.2595\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 1.5470 - accuracy: 0.3977 - val_loss: 1.7360 - val_accuracy: 0.2215\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 1.5524 - accuracy: 0.4659 - val_loss: 1.7049 - val_accuracy: 0.2848\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 1.5860 - accuracy: 0.3636 - val_loss: 1.7579 - val_accuracy: 0.2532\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 1.5803 - accuracy: 0.3523 - val_loss: 1.7179 - val_accuracy: 0.2215\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 1.5665 - accuracy: 0.3409 - val_loss: 1.8114 - val_accuracy: 0.2595\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - 1s 281ms/step - loss: 1.6665 - accuracy: 0.2955 - val_loss: 1.7696 - val_accuracy: 0.2658\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 1.5119 - accuracy: 0.4545 - val_loss: 1.7093 - val_accuracy: 0.2722\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_11 (Conv2D)          (None, 6, 8, 32)          589856    \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 3, 4, 32)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 384)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 639910 (2.44 MB)\n",
            "Trainable params: 639910 (2.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###model training under random sampled frames"
      ],
      "metadata": {
        "id": "gT7DluyIqR9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_base = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate'\n",
        "feature_dir2 = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train/features/features2.npy'\n",
        "labels_dir2 = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/train/features/labels2.npy'\n",
        "val_features2 = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/features/features2.npy'\n",
        "val_labels2 = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/features/labels2.npy'\n",
        "X_train2 = np.load(feature_dir2, allow_pickle = True)\n",
        "y_train2 = np.load(labels_dir2, allow_pickle = True)\n",
        "X_val2 = np.load(val_features2, allow_pickle = True)\n",
        "y_val2 = np.load(val_labels2, allow_pickle = True)"
      ],
      "metadata": {
        "id": "C32nkGu2qXLq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features2 = np.vstack([X_train2, X_val2])\n",
        "# print(y_train)\n",
        "# print(y_val)\n",
        "labels2 = np.hstack([y_train2, y_val2])\n",
        "# print(labels.shape)\n",
        "X_train2, X_val2, y_train2, y_val2 = train_test_split(features2, labels2, test_size=test_size, shuffle = shuffle, random_state=42)"
      ],
      "metadata": {
        "id": "xJDw5JsclemS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model with 3D convolutional layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=X_train2.shape[1:]),  # Input shape matches your feature shape\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')  # Output layer with the number of classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train2, y_train2, epochs=epoch, validation_data=(X_val2, y_val2))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sROBdJP9qi5Y",
        "outputId": "5c6c8eea-e0c6-419f-9651-d6538d5a4fd6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3/3 [==============================] - 2s 457ms/step - loss: 3.7226 - accuracy: 0.2045 - val_loss: 2.6515 - val_accuracy: 0.1709\n",
            "Epoch 2/30\n",
            "3/3 [==============================] - 1s 311ms/step - loss: 2.5663 - accuracy: 0.1591 - val_loss: 2.0107 - val_accuracy: 0.1456\n",
            "Epoch 3/30\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 2.0057 - accuracy: 0.0909 - val_loss: 1.8814 - val_accuracy: 0.2025\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - 1s 290ms/step - loss: 1.9439 - accuracy: 0.1364 - val_loss: 1.7991 - val_accuracy: 0.1709\n",
            "Epoch 5/30\n",
            "3/3 [==============================] - 1s 519ms/step - loss: 1.7906 - accuracy: 0.2045 - val_loss: 1.9240 - val_accuracy: 0.1835\n",
            "Epoch 6/30\n",
            "3/3 [==============================] - 1s 580ms/step - loss: 1.8703 - accuracy: 0.2159 - val_loss: 1.8812 - val_accuracy: 0.1456\n",
            "Epoch 7/30\n",
            "3/3 [==============================] - 1s 517ms/step - loss: 1.7629 - accuracy: 0.2500 - val_loss: 1.7781 - val_accuracy: 0.2532\n",
            "Epoch 8/30\n",
            "3/3 [==============================] - 1s 352ms/step - loss: 1.7820 - accuracy: 0.1932 - val_loss: 1.7794 - val_accuracy: 0.2089\n",
            "Epoch 9/30\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 1.7781 - accuracy: 0.2273 - val_loss: 1.7771 - val_accuracy: 0.2278\n",
            "Epoch 10/30\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 1.7517 - accuracy: 0.2727 - val_loss: 1.8134 - val_accuracy: 0.1772\n",
            "Epoch 11/30\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 1.7351 - accuracy: 0.2614 - val_loss: 1.8015 - val_accuracy: 0.1582\n",
            "Epoch 12/30\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 1.7253 - accuracy: 0.2727 - val_loss: 1.7604 - val_accuracy: 0.3608\n",
            "Epoch 13/30\n",
            "3/3 [==============================] - 1s 315ms/step - loss: 1.7275 - accuracy: 0.3182 - val_loss: 1.7520 - val_accuracy: 0.2722\n",
            "Epoch 14/30\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 1.7222 - accuracy: 0.2614 - val_loss: 1.7424 - val_accuracy: 0.2975\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - 1s 285ms/step - loss: 1.6666 - accuracy: 0.3977 - val_loss: 1.8033 - val_accuracy: 0.3038\n",
            "Epoch 16/30\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 1.6768 - accuracy: 0.3409 - val_loss: 1.7377 - val_accuracy: 0.2532\n",
            "Epoch 17/30\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 1.6518 - accuracy: 0.3750 - val_loss: 1.7312 - val_accuracy: 0.3228\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - 1s 297ms/step - loss: 1.6434 - accuracy: 0.3636 - val_loss: 1.7328 - val_accuracy: 0.3101\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 1.6267 - accuracy: 0.3977 - val_loss: 1.6996 - val_accuracy: 0.3354\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - 1s 480ms/step - loss: 1.6085 - accuracy: 0.4659 - val_loss: 1.7039 - val_accuracy: 0.3101\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - 1s 516ms/step - loss: 1.5984 - accuracy: 0.3864 - val_loss: 1.7099 - val_accuracy: 0.3101\n",
            "Epoch 22/30\n",
            "3/3 [==============================] - 1s 510ms/step - loss: 1.6051 - accuracy: 0.3409 - val_loss: 1.6916 - val_accuracy: 0.3291\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - 1s 575ms/step - loss: 1.5695 - accuracy: 0.4091 - val_loss: 1.6799 - val_accuracy: 0.2911\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 1.5560 - accuracy: 0.4205 - val_loss: 1.6891 - val_accuracy: 0.2722\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 1.5512 - accuracy: 0.3523 - val_loss: 1.6773 - val_accuracy: 0.3101\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 1.5406 - accuracy: 0.4091 - val_loss: 1.6494 - val_accuracy: 0.3544\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 1.5355 - accuracy: 0.4886 - val_loss: 1.7099 - val_accuracy: 0.3228\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - 1s 301ms/step - loss: 1.5738 - accuracy: 0.3523 - val_loss: 1.6686 - val_accuracy: 0.3228\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - 1s 299ms/step - loss: 1.5749 - accuracy: 0.3523 - val_loss: 1.6458 - val_accuracy: 0.3354\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - 1s 288ms/step - loss: 1.5473 - accuracy: 0.4205 - val_loss: 1.7095 - val_accuracy: 0.3038\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 6, 8, 32)          589856    \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 3, 4, 32)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 384)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 639910 (2.44 MB)\n",
            "Trainable params: 639910 (2.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq55TUad9LJR"
      },
      "source": [
        "### Discuss the pros and cons of the type of classifier selected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6B1EtDk9SvG"
      },
      "source": [
        "## evaluate the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "CC3DscKI9fvn"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# # Assuming you have a trained classifier, such as svm_classifier, as described in Section 3\n",
        "# # X_val and y_val are the validation features and labels\n",
        "\n",
        "# # Make predictions on the validation set\n",
        "# y_pred = svm_classifier.predict(X_val)\n",
        "\n",
        "# # Calculate evaluation metrics\n",
        "# accuracy = accuracy_score(y_val, y_pred)\n",
        "# precision = precision_score(y_val, y_pred)\n",
        "# recall = recall_score(y_val, y_pred)\n",
        "# f1 = f1_score(y_val, y_pred)\n",
        "# confusion = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "# # Print the results\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# print(\"F1 Score:\", f1)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5gousOg9rRk"
      },
      "source": [
        "### You should repeat steps 1 and 2 for the validation videos to obtain their features and obtain their class predictions with the trained classifier. Compare the predictions with the ground truth label. What is the performance of the trained classifier? (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2jKYfEbZ9tjI"
      },
      "outputs": [],
      "source": [
        "# # Step 1: Frame Sampling for Validation Videos\n",
        "# # - You can follow the same frame sampling process as in Section 1 for your validation videos.\n",
        "# # - Let's assume you have validation_uniform_frames and validation_random_frames for uniform and random sampling.\n",
        "\n",
        "# # Step 2: Feature Extraction for Validation Videos\n",
        "# # - Apply the same feature extraction process as in Section 2 for the validation videos.\n",
        "# # - Assuming you have a pre-trained model (model) and reference mean and standard deviation (ref_mean, ref_std).\n",
        "\n",
        "# # Extract features for validation videos\n",
        "# validation_uniform_features = preprocess_and_extract_features(validation_uniform_frames, model, ref_mean, ref_std)\n",
        "# validation_random_features = preprocess_and_extract_features(validation_random_frames, model, ref_mean, ref_std)\n",
        "\n",
        "# # Step 3: Classifier Prediction and Evaluation\n",
        "# # - Use the trained classifier to predict classes for validation features and compare with ground truth labels.\n",
        "# # - Assuming you have validation_labels for ground truth labels.\n",
        "\n",
        "# # Predict classes for validation features\n",
        "# validation_uniform_predictions = svm_classifier.predict(validation_uniform_features)\n",
        "# validation_random_predictions = svm_classifier.predict(validation_random_features)\n",
        "\n",
        "# # Evaluate the performance for uniform sampling\n",
        "# uniform_accuracy = accuracy_score(validation_labels, validation_uniform_predictions)\n",
        "# uniform_precision = precision_score(validation_labels, validation_uniform_predictions)\n",
        "# uniform_recall = recall_score(validation_labels, validation_uniform_predictions)\n",
        "# uniform_f1 = f1_score(validation_labels, validation_uniform_predictions)\n",
        "\n",
        "# # Evaluate the performance for random sampling\n",
        "# random_accuracy = accuracy_score(validation_labels, validation_random_predictions)\n",
        "# random_precision = precision_score(validation_labels, validation_random_predictions)\n",
        "# random_recall = recall_score(validation_labels, validation_random_predictions)\n",
        "# random_f1 = f1_score(validation_labels, validation_random_predictions)\n",
        "\n",
        "# # Compare and print the results\n",
        "# print(\"Performance for Uniform Sampling:\")\n",
        "# print(\"Accuracy:\", uniform_accuracy)\n",
        "# print(\"Precision:\", uniform_precision)\n",
        "# print(\"Recall:\", uniform_recall)\n",
        "# print(\"F1 Score:\", uniform_f1)\n",
        "\n",
        "# print(\"\\nPerformance for Random Sampling:\")\n",
        "# print(\"Accuracy:\", random_accuracy)\n",
        "# print(\"Precision:\", random_precision)\n",
        "# print(\"Recall:\", random_recall)\n",
        "# print(\"F1 Score:\", random_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRx-6qmG-BLD"
      },
      "source": [
        "#4. **Apply any image enhancement of your choice** and explore how it effects the performance of the trained classifier. Note that the reference mean, and standard deviation value of a normal video frame is \"mean\" [0.485,0.456,0.406],\"standard deviation\" [0.229,0.224,0.225]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "wI3trLEE-K_1"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# # Function to enhance and normalize a frame\n",
        "# def enhance_and_normalize_frame(frame, ref_mean, ref_std):\n",
        "#     # Enhance the frame (e.g., histogram equalization)\n",
        "#     enhanced_frame = cv2.equalizeHist(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
        "#     enhanced_frame = cv2.cvtColor(enhanced_frame, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "#     # Normalize the frame to have the reference mean and standard deviation\n",
        "#     actual_mean = np.mean(enhanced_frame, axis=(0, 1))\n",
        "#     actual_std = np.std(enhanced_frame, axis=(0, 1))\n",
        "#     normalized_frame = (enhanced_frame - actual_mean) / actual_std * ref_std + ref_mean\n",
        "\n",
        "#     return normalized_frame\n",
        "\n",
        "# # Function to preprocess frames and extract features using a pre-trained model\n",
        "# def preprocess_and_extract_features(frames, model, ref_mean, ref_std):\n",
        "#     processed_frames = [enhance_and_normalize_frame(frame, ref_mean, ref_std) for frame in frames]\n",
        "#     processed_frames = [preprocess_input(frame) for frame in processed_frames]\n",
        "#     features = model.predict(np.array(processed_frames))\n",
        "#     return features\n",
        "\n",
        "# # Assuming you have validation_videos, validation_labels, a pre-trained model (model),\n",
        "# # and a trained classifier (svm_classifier)\n",
        "\n",
        "# # Without Image Enhancement\n",
        "# features_without_enhancement = preprocess_and_extract_features(validation_videos, model, ref_mean=[0.485, 0.456, 0.406], ref_std=[0.229, 0.224, 0.225])\n",
        "# predictions_without_enhancement = svm_classifier.predict(features_without_enhancement)\n",
        "\n",
        "# # With Image Enhancement\n",
        "# features_with_enhancement = preprocess_and_extract_features(validation_videos, model, ref_mean=[0.485, 0.456, 0.406], ref_std=[0.229, 0.224, 0.225])\n",
        "# predictions_with_enhancement = svm_classifier.predict(features_with_enhancement)\n",
        "\n",
        "# # Evaluate the performance without and with image enhancement\n",
        "# accuracy_without_enhancement = accuracy_score(validation_labels, predictions_without_enhancement)\n",
        "# f1_score_without_enhancement = f1_score(validation_labels, predictions_without_enhancement)\n",
        "\n",
        "# accuracy_with_enhancement = accuracy_score(validation_labels, predictions_with_enhancement)\n",
        "# f1_score_with_enhancement = f1_score(validation_labels, predictions_with_enhancement)\n",
        "\n",
        "# # Print the results\n",
        "# print(\"Performance without Image Enhancement:\")\n",
        "# print(\"Accuracy:\", accuracy_without_enhancement)\n",
        "# print(\"F1 Score:\", f1_score_without_enhancement)\n",
        "\n",
        "# print(\"\\nPerformance with Image Enhancement:\")\n",
        "# print(\"Accuracy:\", accuracy_with_enhancement)\n",
        "# print(\"F1 Score:\", f1_score_with_enhancement)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nAuHWBK-Yub"
      },
      "source": [
        "### Discuss how the chosen image enhancement effects the performance of the trained classifier in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D60ckbCL-avy"
      },
      "source": [
        "### Provide sampled output frames resulting from the image enhancement. (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0-Q29E_Ol2"
      },
      "source": [
        "# 5. â€“ Improving the HAR Model to Enable End-to-end Training. The aforementioned method is intuitive but is not end-to-end, which limits its applicability in real-world scenarios. Currently, most HAR models are designed end-to-end, without the need to explicitly store the video features. In this step you are to design or implement an HAR model that is end-to-end and evaluate your HAR model. Describe your HAR model in detail, including the structure along with the training and evaluation procedures. Compare your HAR model performance against the prior trained classifiers and discuss the pros and cons of your HAR model. (Additional 10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "4Cn7kKuH-f2E"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import Conv2D, LSTM, Dense, Flatten, Input\n",
        "# from tensorflow.keras.models import Model\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# # Assuming you have your training data in train_videos and train_labels\n",
        "# # Assuming you have your validation data in validation_videos and validation_labels\n",
        "\n",
        "# # Define the input shape\n",
        "# input_shape = (sequence_length, frame_height, frame_width, num_channels)\n",
        "\n",
        "# # Build the end-to-end HAR model\n",
        "# input_layer = Input(shape=input_shape)\n",
        "# conv_layer = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "# lstm_layer = LSTM(64, return_sequences=True)(conv_layer)\n",
        "# flatten_layer = Flatten()(lstm_layer)\n",
        "# output_layer = Dense(num_classes, activation='softmax')(flatten_layer)\n",
        "\n",
        "# model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(train_videos, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_videos, validation_labels))\n",
        "\n",
        "# # Evaluate the model on the validation set\n",
        "# validation_predictions = model.predict(validation_videos)\n",
        "# validation_predictions = np.argmax(validation_predictions, axis=1)  # Assuming one-hot encoding of labels\n",
        "# validation_labels = np.argmax(validation_labels, axis=1)  # Assuming one-hot encoding of labels\n",
        "\n",
        "# accuracy = accuracy_score(validation_labels, validation_predictions)\n",
        "# precision = precision_score(validation_labels, validation_predictions, average='weighted')\n",
        "# recall = recall_score(validation_labels, validation_predictions, average='weighted')\n",
        "# f1 = f1_score(validation_labels, validation_predictions, average='weighted')\n",
        "\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "MuCiCk387VRN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "C44m4oIVwG2u"
      },
      "outputs": [],
      "source": [
        "# from shutil import rmtree\n",
        "# b = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/random_validation_sampled'\n",
        "# c = '/content/drive/MyDrive/machine vision assignment 2/EE6222 train and validate 2023/validate/uniform_validation_sampled'\n",
        "# # for file in os.listdir(b):\n",
        "# #   os.unlink(os.path.join(b, file))\n",
        "# rmtree(b)\n",
        "\n",
        "# for f in os.listdir(c):\n",
        "#   os.unlink(os.path.join(c, file))\n",
        "  # rmtree(os.path.join(c, f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "rpoAtnLRwM7v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}